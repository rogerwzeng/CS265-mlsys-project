# -*- coding: utf-8 -*-
"""muTwoProfiler-cs.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1SwvAgLJhPozazPid2GIg2gm3Twpe9pol

!pip install wandb onnx datasets -Uq
!pip install --upgrade torchvision torchaudio --index-url https://download.pytorch.org/whl/cu118
"""

import numpy as np
import torch
from torch.profiler import profile, ProfilerActivity, ProfilerAction, schedule
from dataclasses import dataclass
from typing import Optional, List, Dict
import matplotlib.pyplot as plt
import torchvision
import torchvision.models as models
from transformers import BertModel

import wandb
# from google.colab import userdata
# wb_api_key = userdata.get('WANDB_API_KEY')
# wandb.login(key = wb_api_key)
wandb.login(key="2c25567b01802223eadb2ec1e05e197fe683fc72")


@dataclass
class ProfileStats:
    batch_size: int
    avg_latency: float = 0.0
    peak_memory: float = 0.0
    throughput: float = 0.0
    kernel_activities: Dict = None


class ModelProfiler(profile):
    """
    Extended torch.profiler.profile for comprehensive model profiling
    """
    def __init__(
        self,
        model: torch.nn.Module,
        warmup: int = 1,
        steps: int = 10,
        batch_sizes: List[int] = None,
        use_fake_tensors: bool = False
    ):
        # Initialize parent profiler with common settings
        super().__init__(activities=[ProfilerActivity.CPU, ProfilerActivity.CUDA],
                schedule=schedule(wait=0, warmup=warmup, active=steps, repeat=1),
                record_shapes=True,
                with_stack=True,
                profile_memory=True,
                with_flops=True)

        if not torch.cuda.is_available():
            raise RuntimeError("CUDA device required for profiling")

        self.device = 'cuda'
        self.model = model.cuda()
        self.model = torch.compile(
            self.model,
            mode='default',
            fullgraph=False,
            backend='aot_eager'  # prefer eager for AOTAutograd over inductor
        )
        self.use_fake_tensors = use_fake_tensors
        self.warmup = warmup
        self.steps = steps
        self.batch_sizes = batch_sizes or [1, 2, 4, 8, 16, 32]
        self.results: Dict[int, ProfileStats] = {}
        self.transfer_stats = {}

    def _create_pinned_tensor(self, size, dtype=torch.float32):
        """Safe allocation of pinned memory with fallback to non-pinned allocation"""
        try:
            return torch.empty(size, dtype=dtype, pin_memory=True)
        except RuntimeError as e:
            print(f"Warning: Failed to allocate pinned memory: {e}")
            print("Falling back to regular memory")
            return torch.empty(size, dtype=dtype)

    def _create_fake_inputs(self, batch_size: int):

        """Create fake tensors for memory-efficient profiling"""
        if hasattr(self.model, 'config') and hasattr(self.model.config, 'model_type'):
            # BERT fake input tensor
            # If torch.fake_tensor is not available, use torch.empty instead
            if hasattr(torch, 'fake_tensor'): # Check if fake_tensor is available
                return {
                    'input_ids': torch.fake_tensor( (batch_size, 128), dtype=torch.long, device='cuda'),
                    'attention_mask': torch.fake_tensor( (batch_size, 128), dtype=torch.float, device='cuda')
                }
            else:
                return {
                    'input_ids': torch.empty( (batch_size, 128), dtype=torch.long, device='cuda'),
                    'attention_mask': torch.empty( (batch_size, 128), dtype=torch.float, device='cuda')
                }
        else:
            # ResNET fake input tensor
            # If torch.fake_tensor is not available, use torch.empty instead
            if hasattr(torch, 'fake_tensor'): # Check if fake_tensor is available
                return torch.fake_tensor( (batch_size, 3, 224, 224), dtype=torch.float, device='cuda')
            else:
                return torch.empty( (batch_size, 3, 224, 224), dtype=torch.float, device='cuda')

    def _create_real_inputs(self, batch_size: int):
        """Create real tensors for actual profiling using pinned memory"""
        if hasattr(self.model, 'config') and hasattr(self.model.config, 'model_type'):
            # Transformer model inputs - using pinned memory
            return {
                'input_ids': self._create_pinned_tensor( (batch_size, 128), dtype=torch.long).random_(0, 30522),
                'attention_mask': self._create_pinned_tensor( (batch_size, 128), dtype=torch.float).fill_(1)
            }
        else:
            # Vision model inputs - using pinned memory
            return self._create_pinned_tensor( (batch_size, 3, 224, 224), dtype=torch.float).normal_(0, 1)


    def _create_transfer_events(self):
        """Create CUDA events for timing transfers"""
        return {
            'start': torch.cuda.Event(enable_timing=True),
            'end': torch.cuda.Event(enable_timing=True)
        }

    def _measure_transfer(self, tensor, transfer_type='H2D'):
        """Measure transfer time for a single tensor"""
        events = self._create_transfer_events()

        # Ensure input tensor is pinned if it's not already
        if not tensor.is_pinned() and transfer_type == 'H2D':
            tensor = tensor.pin_memory()

        events['start'].record()
        if transfer_type == 'H2D':
            # Host to Device transfer
            tensor_gpu = tensor.cuda(non_blocking=True) # for pinned memory, use non-blocking transfer
        else:
            # Device to Host transfer
            tensor_cpu = torch.empty_like(tensor, device='cpu', pin_memory=True)
            tensor_cpu.copy_(tensor, non_blocking=True)  # Non-blocking copy

        torch.cuda.synchronize()
        events['end'].record()

        torch.cuda.synchronize()
        transfer_time = events['start'].elapsed_time(events['end'])
        return transfer_time, tensor_gpu if transfer_type == 'H2D' else tensor_cpu

    # Main profiling function
    # NOTE:
    # 1. we are only profiling the forward pass
    # 2. "steps" is the number of repetitions
    #
    def profile_batch(self, batch_size: int) -> ProfileStats:
        """Profile model with specific batch size"""
        stats = ProfileStats(batch_size=batch_size)
        transfer_times = {'H2D': [], 'D2H': []}

        # Allocate a memory pool for pinned memory
        with torch.cuda.device(0):
            torch.cuda.empty_cache()
            # Optional: Set memory pool for pinned allocations
            if hasattr(torch.cuda, 'memory_pool'):
                torch.cuda.memory_pool(device='cpu').empty_cache()

        # First pass with fake tensors to check memory requirements
        if self.use_fake_tensors:
            with torch.autograd.set_grad_enabled(False):
                fake_inputs = self._create_fake_inputs(batch_size)
                try:
                    # Get memory estimate using fake tensors
                    with torch.amp.autocast('cuda'):
                        self.model(**fake_inputs if isinstance(fake_inputs, dict) else fake_inputs)
                except Exception as e:
                    print(f"Memory estimation failed: {e}")
                    return None

        # Reset CUDA stats
        torch.cuda.reset_peak_memory_stats()
        torch.cuda.empty_cache()

        # Create real inputs for actual profiling
        inputs = self._create_real_inputs(batch_size)

        # Measure H2D transfer of inputs
        if isinstance(inputs, dict):
            h2d_times = {}
            gpu_inputs = {}
            for k, v in inputs.items():
                transfer_time, gpu_tensor = self._measure_transfer(v, 'H2D')
                h2d_times[k] = transfer_time
                gpu_inputs[k] = gpu_tensor
            transfer_times['H2D'].append(('input_tensors', h2d_times))
            inputs = gpu_inputs
        else:
            transfer_time, inputs = self._measure_transfer(inputs, 'H2D')
            transfer_times['H2D'].append(('input_tensor', transfer_time))

        # Start profiling
        self.start()

        try:
            for step in range(self.warmup + self.steps):
                with torch.no_grad(), torch.amp.autocast('cuda'):
                    outputs = self.model(**inputs if isinstance(inputs, dict) else inputs)

                    # Measure D2H transfer of outputs
                    if isinstance(outputs, dict):
                        d2h_times = {}
                        for k, v in outputs.items():
                            transfer_time, _ = self._measure_transfer(v, 'D2H')
                            d2h_times[k] = transfer_time
                        transfer_times['D2H'].append((f'output_tensors_step_{step}', d2h_times))
                    else:
                        transfer_time, _ = self._measure_transfer(outputs, 'D2H')
                        transfer_times['D2H'].append((f'output_tensor_step_{step}', transfer_time))

                self.step()
        finally:
            self.stop()

        # Get swap-in & swap-out transfer statistics
        stats.transfer_stats = {
            'H2D': {
                'total_time': sum(t[1] if isinstance(t[1], float) else sum(t[1].values())
                                for t in transfer_times['H2D']),
                'details': transfer_times['H2D']
            },
            'D2H': {
                'total_time': sum(t[1] if isinstance(t[1], float) else sum(t[1].values())
                                for t in transfer_times['D2H']),
                'details': transfer_times['D2H']
            }
        }

        # Analyze the rest of profiler results
        events = self.key_averages()
        print(events.table(sort_by="self_cuda_time_total", row_limit=3))

        # Calculate statistics
        stats.peak_memory = torch.cuda.max_memory_allocated() / 1e6  # MB
        stats.avg_latency = sum(e.cuda_time_total for e in events) / self.steps
        stats.avg_latency = sum(sum(ev.cuda_time_total for ev in e.events) for e in events) / self.steps
        stats.throughput = batch_size * self.steps / (stats.avg_latency / 1000)  # samples/second
        stats.kernel_activities = {
            e.key: {
                'cuda_time': e.cuda_time_total,
                'cuda_memory': e.cuda_memory_usage,
                'self_cuda_time': e.self_cuda_time_total
            }
            for e in events
        }

        return stats

    # Outer loop through batch sizes
    def run_analysis(self):
        """Run profiling for all batch sizes"""
        for batch_size in self.batch_sizes:
            try:
                print(f"\nProfiling batch size: {batch_size}")
                stats = self.profile_batch(batch_size)
                self.results[batch_size] = stats
                if stats is None:
                    print(f"Skipping batch size {batch_size} due to memory constraints")
                    continue

                # Print summary
                print(f"Average Latency: {stats.avg_latency:.2f}ms")
                print(f"Peak Memory: {stats.peak_memory:.2f}MB")
                print(f"Throughput: {stats.throughput:.2f} samples/sec")

                # Print top kernel activities
                print("\nTop 5 CUDA kernels by time:")
                sorted_kernels = sorted(stats.kernel_activities.items(), key=lambda x: x[1]['cuda_time'], reverse=True)[:5]
                for kernel, metrics in sorted_kernels:
                    print(f"{kernel}: {metrics['cuda_time']:.2f}ms")

                # Print transfer statistics
                if  hasattr(stats, 'transfer_stats'):
                    print("\nTransfer Statistics:")
                    print(f"Total H2D Time: {stats.transfer_stats['H2D']['total_time']:.2f}ms")
                    print(f"Total D2H Time: {stats.transfer_stats['D2H']['total_time']:.2f}ms")

                    print("\nDetailed H2D Transfers:")
                    for name, time in stats.transfer_stats['H2D']['details']:
                        if isinstance(time, dict):
                            for k, v in time.items():
                                print(f"{name} - {k}: {v:.2f}ms")
                        else:
                            print(f"{name}: {time:.2f}ms")

                    print("\nDetailed D2H Transfers:")
                    for name, time in stats.transfer_stats['D2H']['details']:
                        if isinstance(time, dict):
                            for k, v in time.items():
                                print(f"{name} - {k}: {v:.2f}ms")
                        else:
                            print(f"{name}: {time:.2f}ms")

            except RuntimeError as e:
                print(f"Error profiling batch size {batch_size}: {e}")
                break

    def plot_results(self):
        """Visualize profiling results"""
        if not self.results:
            print("No results to plot")
            return

        batch_sizes = list(self.results.keys())
        latencies = [stats.avg_latency for stats in self.results.values()]
        memories = [stats.peak_memory for stats in self.results.values()]
        throughputs = [stats.throughput for stats in self.results.values()]

        # Extract transfer times
        h2d_times = [stats.transfer_stats['H2D']['total_time'] for stats in self.results.values()]
        d2h_times = [stats.transfer_stats['D2H']['total_time'] for stats in self.results.values()]

        # Create subplot grid
        fig, ((ax1, ax2), (ax3, ax4)) = plt.subplots(2, 2, figsize=(15, 10))

        # Latency plot
        ax1.plot(batch_sizes, latencies, 'b-o')
        ax1.set_xlabel('Batch Size')
        ax1.set_ylabel('Latency (ms)')
        ax1.set_title('Latency vs Batch Size')

        # Memory plot
        ax2.plot(batch_sizes, memories, 'r-o')
        ax2.set_xlabel('Batch Size')
        ax2.set_ylabel('Peak Memory (MB)')
        ax2.set_title('Memory Usage vs Batch Size')

        # Throughput plot
        ax3.plot(batch_sizes, throughputs, 'g-o')
        ax3.set_xlabel('Batch Size')
        ax3.set_ylabel('Throughput (samples/sec)')
        ax3.set_title('Throughput vs Batch Size')

        # Transfer times plot
        ax4.plot(batch_sizes, h2d_times, 'c-o', label='H2D Transfer')
        ax4.plot(batch_sizes, d2h_times, 'm-o', label='D2H Transfer')
        ax4.set_xlabel('Batch Size')
        ax4.set_ylabel('Transfer Time (ms)')
        ax4.set_title('Memory Transfer Times vs Batch Size')
        ax4.legend()
        ax4.grid(True)

        # Second y-axis for total transfer time
        ax4_twin = ax4.twinx()
        total_transfer = [h + d for h, d in zip(h2d_times, d2h_times)]
        ax4_twin.plot(batch_sizes, total_transfer, 'k--', label='Total Transfer')
        ax4_twin.set_ylabel('Total Transfer Time (ms)')
        ax4_twin.legend(loc='center right')

        plt.tight_layout()
        plt.show()

        # Detailed transfer time breakdown plot
        plt.figure(figsize=(10, 6))
        width = 0.35
        x = np.arange(len(batch_sizes))

        plt.bar(x - width/2, h2d_times, width, label='H2D Transfer', color='c')
        plt.bar(x + width/2, d2h_times, width, label='D2H Transfer', color='m')

        plt.xlabel('Batch Size')
        plt.ylabel('Transfer Time (ms)')
        plt.title('Memory Transfer Time Breakdown')
        plt.xticks(x, batch_sizes)
        plt.legend()
        plt.grid(True, alpha=0.3)

        # Value labels on top of each bar
        for i, (h2d, d2h) in enumerate(zip(h2d_times, d2h_times)):
            plt.text(i - width/2, h2d, f'{h2d:.1f}', ha='center', va='bottom')
            plt.text(i + width/2, d2h, f'{d2h:.1f}', ha='center', va='bottom')

        plt.tight_layout()
        plt.show()

# For BERT
import os

def profile_bert():
    model = BertModel.from_pretrained('bert-base-uncased')
    profiler = ModelProfiler(
        model,
        warmup=3,
        steps=10,
        batch_sizes=[1, 2, 4, 8, 16, 32, 64],
        use_fake_tensors=True
    )
    profiler.run_analysis()
    profiler.plot_results()

# For RESNET
def profile_resnet():
    model = models.resnet152(weights=models.ResNet152_Weights.IMAGENET1K_V1)

    profiler = ModelProfiler(
        model,
        warmup=3,
        steps=10,
        batch_sizes=[1, 2, 4, 8, 16, 32, 64],
        use_fake_tensors=True
    )
    profiler.run_analysis()
    profiler.plot_results()

if __name__ == "__main__":
    profile_resnet()
    profile_bert()
